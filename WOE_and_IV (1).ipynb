{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WOE and IV",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxOQcsM151re"
      },
      "source": [
        "## WOE (Weight of Evidence)\n",
        "\n",
        "What is Weight of Evidence (WOE)?\n",
        "The weight of evidence tells the predictive power of an independent variable in relation to the dependent variable. It's good to understand the concept of WOE in terms of events and non-events. It is calculated by taking the natural logarithm (log to base e) of division of % of non-events and % of events.\n",
        "\n",
        "WOE = In(% of non-events ➗ % of events)\n",
        "\n",
        "## Information Value (IV)\n",
        "\n",
        "Information value is one of the most useful technique to select important variables in a predictive model. It helps to rank variables on the basis of their importance. The IV is calculated using the following formula :\n",
        "\n",
        "IV = ∑ (% of non-events - % of events) * WOE\n",
        "\n",
        "\n",
        "Information Value\tVariable Predictiveness\n",
        "\n",
        "\n",
        "*   Less than 0.02\tNot useful for prediction\n",
        "*   0.02 to 0.1\tWeak predictive Power\n",
        "*   0.1 to 0.3\tMedium predictive Power\n",
        "*   0.3 to 0.5\tStrong predictive Power\n",
        "*   \">0.5\tSuspicious Predictive Power\n",
        "\n",
        "Read More here:\n",
        "https://www.listendata.com/2015/03/weight-of-evidence-woe-and-information.html\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROfNXLTSBAE_"
      },
      "source": [
        "import pandas.core.algorithms as algos\n",
        "from pandas import Series\n",
        "import scipy.stats.stats as stats\n",
        "import re\n",
        "import traceback\n",
        "import string\n",
        "\n",
        "max_bin = 20\n",
        "force_bin = 3\n",
        "\n",
        "# define a binning function\n",
        "def mono_bin(Y, X, n = max_bin):\n",
        "    \n",
        "    df1 = pd.DataFrame({\"X\": X, \"Y\": Y})\n",
        "    justmiss = df1[['X','Y']][df1.X.isnull()]\n",
        "    notmiss = df1[['X','Y']][df1.X.notnull()]\n",
        "    r = 0\n",
        "    while np.abs(r) < 1:\n",
        "        try:\n",
        "            d1 = pd.DataFrame({\"X\": notmiss.X, \"Y\": notmiss.Y, \"Bucket\": pd.qcut(notmiss.X, n)})\n",
        "            d2 = d1.groupby('Bucket', as_index=True)\n",
        "            r, p = stats.spearmanr(d2.mean().X, d2.mean().Y)\n",
        "            n = n - 1 \n",
        "        except Exception as e:\n",
        "            n = n - 1\n",
        "\n",
        "    if len(d2) == 1:\n",
        "        n = force_bin         \n",
        "        bins = algos.quantile(notmiss.X, np.linspace(0, 1, n))\n",
        "        if len(np.unique(bins)) == 2:\n",
        "            bins = np.insert(bins, 0, 1)\n",
        "            bins[1] = bins[1]-(bins[1]/2)\n",
        "        d1 = pd.DataFrame({\"X\": notmiss.X, \"Y\": notmiss.Y, \"Bucket\": pd.cut(notmiss.X, np.unique(bins),include_lowest=True)}) \n",
        "        d2 = d1.groupby('Bucket', as_index=True)\n",
        "    \n",
        "    d3 = pd.DataFrame({},index=[])\n",
        "    d3[\"MIN_VALUE\"] = d2.min().X\n",
        "    d3[\"MAX_VALUE\"] = d2.max().X\n",
        "    d3[\"COUNT\"] = d2.count().Y\n",
        "    d3[\"EVENT\"] = d2.sum().Y\n",
        "    d3[\"NONEVENT\"] = d2.count().Y - d2.sum().Y\n",
        "    d3=d3.reset_index(drop=True)\n",
        "    \n",
        "    if len(justmiss.index) > 0:\n",
        "        d4 = pd.DataFrame({'MIN_VALUE':np.nan},index=[0])\n",
        "        d4[\"MAX_VALUE\"] = np.nan\n",
        "        d4[\"COUNT\"] = justmiss.count().Y\n",
        "        d4[\"EVENT\"] = justmiss.sum().Y\n",
        "        d4[\"NONEVENT\"] = justmiss.count().Y - justmiss.sum().Y\n",
        "        d3 = d3.append(d4,ignore_index=True)\n",
        "    \n",
        "    d3[\"EVENT_RATE\"] = d3.EVENT/d3.COUNT\n",
        "    d3[\"NON_EVENT_RATE\"] = d3.NONEVENT/d3.COUNT\n",
        "    d3[\"DIST_EVENT\"] = d3.EVENT/d3.sum().EVENT\n",
        "    d3[\"DIST_NON_EVENT\"] = d3.NONEVENT/d3.sum().NONEVENT\n",
        "    d3[\"WOE\"] = np.log(d3.DIST_EVENT/d3.DIST_NON_EVENT)\n",
        "    d3[\"IV\"] = (d3.DIST_EVENT-d3.DIST_NON_EVENT)*np.log(d3.DIST_EVENT/d3.DIST_NON_EVENT)\n",
        "    d3[\"VAR_NAME\"] = \"VAR\"\n",
        "    d3 = d3[['VAR_NAME','MIN_VALUE', 'MAX_VALUE', 'COUNT', 'EVENT', 'EVENT_RATE', 'NONEVENT', 'NON_EVENT_RATE', 'DIST_EVENT','DIST_NON_EVENT','WOE', 'IV']]       \n",
        "    d3 = d3.replace([np.inf, -np.inf], 0)\n",
        "    d3.IV = d3.IV.sum()\n",
        "    \n",
        "    return(d3)\n",
        "\n",
        "def char_bin(Y, X):\n",
        "        \n",
        "    df1 = pd.DataFrame({\"X\": X, \"Y\": Y})\n",
        "    justmiss = df1[['X','Y']][df1.X.isnull()]\n",
        "    notmiss = df1[['X','Y']][df1.X.notnull()]    \n",
        "    df2 = notmiss.groupby('X',as_index=True)\n",
        "    \n",
        "    d3 = pd.DataFrame({},index=[])\n",
        "    d3[\"COUNT\"] = df2.count().Y\n",
        "    d3[\"MIN_VALUE\"] = df2.sum().Y.index\n",
        "    d3[\"MAX_VALUE\"] = d3[\"MIN_VALUE\"]\n",
        "    d3[\"EVENT\"] = df2.sum().Y\n",
        "    d3[\"NONEVENT\"] = df2.count().Y - df2.sum().Y\n",
        "    \n",
        "    if len(justmiss.index) > 0:\n",
        "        d4 = pd.DataFrame({'MIN_VALUE':np.nan},index=[0])\n",
        "        d4[\"MAX_VALUE\"] = np.nan\n",
        "        d4[\"COUNT\"] = justmiss.count().Y\n",
        "        d4[\"EVENT\"] = justmiss.sum().Y\n",
        "        d4[\"NONEVENT\"] = justmiss.count().Y - justmiss.sum().Y\n",
        "        d3 = d3.append(d4,ignore_index=True)\n",
        "    \n",
        "    d3[\"EVENT_RATE\"] = d3.EVENT/d3.COUNT\n",
        "    d3[\"NON_EVENT_RATE\"] = d3.NONEVENT/d3.COUNT\n",
        "    d3[\"DIST_EVENT\"] = d3.EVENT/d3.sum().EVENT\n",
        "    d3[\"DIST_NON_EVENT\"] = d3.NONEVENT/d3.sum().NONEVENT\n",
        "    d3[\"WOE\"] = np.log(d3.DIST_EVENT/d3.DIST_NON_EVENT)\n",
        "    d3[\"IV\"] = (d3.DIST_EVENT-d3.DIST_NON_EVENT)*np.log(d3.DIST_EVENT/d3.DIST_NON_EVENT)\n",
        "    d3[\"VAR_NAME\"] = \"VAR\"\n",
        "    d3 = d3[['VAR_NAME','MIN_VALUE', 'MAX_VALUE', 'COUNT', 'EVENT', 'EVENT_RATE', 'NONEVENT', 'NON_EVENT_RATE', 'DIST_EVENT','DIST_NON_EVENT','WOE', 'IV']]      \n",
        "    d3 = d3.replace([np.inf, -np.inf], 0)\n",
        "    d3.IV = d3.IV.sum()\n",
        "    d3 = d3.reset_index(drop=True)\n",
        "    \n",
        "    return(d3)\n",
        "\n",
        "def data_vars(df1, target):\n",
        "    \n",
        "    stack = traceback.extract_stack()\n",
        "    filename, lineno, function_name, code = stack[-2]\n",
        "    vars_name = re.compile(r'\\((.*?)\\).*$').search(code).groups()[0]\n",
        "    final = (re.findall(r\"[\\w']+\", vars_name))[-1]\n",
        "    \n",
        "    x = df1.dtypes.index\n",
        "    count = -1\n",
        "    \n",
        "    for i in x:\n",
        "        if i.upper() not in (final.upper()):\n",
        "            if np.issubdtype(df1[i], np.number) and len(Series.unique(df1[i])) > 2:\n",
        "                conv = mono_bin(target, df1[i])\n",
        "                conv[\"VAR_NAME\"] = i\n",
        "                count = count + 1\n",
        "            else:\n",
        "                conv = char_bin(target, df1[i])\n",
        "                conv[\"VAR_NAME\"] = i            \n",
        "                count = count + 1\n",
        "                \n",
        "            if count == 0:\n",
        "                iv_df = conv\n",
        "            else:\n",
        "                iv_df = iv_df.append(conv,ignore_index=True)\n",
        "    \n",
        "    iv = pd.DataFrame({'IV':iv_df.groupby('VAR_NAME').IV.max()})\n",
        "    iv = iv.reset_index()\n",
        "    return(iv_df,iv)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHLoQcNbB5MP"
      },
      "source": [
        "final_iv, IV = data_vars(input_df,input_df.target)\n",
        "final_iv.to_excel(\"woe.xlsx\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}